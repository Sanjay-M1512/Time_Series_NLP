{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Install it using:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    sys.exit(1)\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "filename = input(\"Enter text file name for full text processing: \")\n",
    "filepath = os.path.join(sys.path[0], filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"\\nOriginal Text Sample:\")\n",
    "print(content[:300])\n",
    "print()\n",
    "\n",
    "print(\"=== Lemmatization: Individual Words ===\")\n",
    "sample_words = \"friendship studied was am is organizing matches\"\n",
    "doc = nlp(sample_words)\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_space:\n",
    "        print(f\"{token.text} -> {token.lemma_}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Stemming: Individual Words ===\")\n",
    "for word in sample_words.split():\n",
    "    print(f\"{word} --> {stemmer.stem(word)}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Lemmatization: Full Text ===\")\n",
    "doc_full = nlp(content)\n",
    "tokens = [token for token in doc_full if not token.is_space][:50]\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"{token.text} --> {token.lemma_}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Stemming: Full Text ===\")\n",
    "for token in tokens:\n",
    "    print(f\"{token.text} --> {stemmer.stem(token.text.lower())}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Practice 6.2: Lemmatization vs Stemming ===\")\n",
    "print(\"Word\\t\\tLemma\\t\\tStem\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "practice_words = \"running good universities flies fairer is\"\n",
    "doc_practice = nlp(practice_words)\n",
    "\n",
    "for token in doc_practice:\n",
    "    word = token.text\n",
    "    lemma = token.lemma_\n",
    "    stem = stemmer.stem(word.lower())\n",
    "    print(f\"{word}\\t\\t{lemma}\\t\\t{stem}\")\n",
    "print()\n",
    "\n",
    "# ------------------ 7. CONCLUSION ------------------\n",
    "print(\"Conclusion:\")\n",
    "print(\"Lemmatization produces dictionary-based meaningful root words, while stemming may distort words by chopping suffixes. For NLP tasks like search, topic modeling, and information retrieval, lemmatization gives better and cleaner output.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
