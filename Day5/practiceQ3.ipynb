{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Install it using:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    sys.exit(1)\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "filename = input(\"Enter text file name: \")\n",
    "\n",
    "filepath = os.path.join(sys.path[0], filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(\"\\nOriginal Text Sample:\")\n",
    "print(content[:300])\n",
    "print()\n",
    "\n",
    "doc = nlp(content)\n",
    "tokens = [token for token in doc if not token.is_space]\n",
    "token_texts = [token.text for token in tokens]\n",
    "\n",
    "print(f\"Total Tokens Count: {len(tokens)}\")\n",
    "print()\n",
    "\n",
    "lemmas = [token.lemma_ for token in tokens]\n",
    "\n",
    "print(\"=== Lemmatized Sample (First 20 tokens) ===\")\n",
    "print(lemmas[:20])\n",
    "print()\n",
    "\n",
    "print(\"Word --> Lemma\")\n",
    "for word, lemma in zip(token_texts[:30], lemmas[:30]):\n",
    "    print(f\"{word} --> {lemma}\")\n",
    "print()\n",
    "\n",
    "stems = [stemmer.stem(token.text.lower()) for token in tokens]\n",
    "\n",
    "print(\"=== Stemmed Sample (First 20 tokens) ===\")\n",
    "print(stems[:20])\n",
    "print()\n",
    "\n",
    "print(\"Word --> Stem\")\n",
    "for word, stem in zip(token_texts[:30], stems[:30]):\n",
    "    print(f\"{word} --> {stem}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Comparison: Lemmatization vs Stemming ===\")\n",
    "print(\"Word\\t\\tLemma\\t\\tStem\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "for word, lemma, stem in zip(token_texts[:30], lemmas[:30], stems[:30]):\n",
    "    print(f\"{word}\\t\\t{lemma}\\t\\t{stem}\")\n",
    "print()\n",
    "\n",
    "print(\"Conclusion:\")\n",
    "print(\n",
    "\"Lemmatization produces dictionary-based meaningful root words, \"\n",
    "\"while stemming may distort words by chopping suffixes. \"\n",
    "\"For NLP tasks like search, topic modeling, and information retrieval, \"\n",
    "\"lemmatization gives better and cleaner output.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
